{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fe51ead-d497-475b-8a4c-fb60e6cdcfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "646952ed-0ffe-4105-bed8-fb18dea1d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./최종데이터.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12cf84b6-08ae-4e4b-b622-ad1b0f3dd45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>height</th>\n",
       "      <th>chest</th>\n",
       "      <th>waist</th>\n",
       "      <th>thigh</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>arm</th>\n",
       "      <th>top</th>\n",
       "      <th>bottom</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1778</td>\n",
       "      <td>892</td>\n",
       "      <td>720</td>\n",
       "      <td>533</td>\n",
       "      <td>441.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>443</td>\n",
       "      <td>979</td>\n",
       "      <td>60.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1702</td>\n",
       "      <td>1021</td>\n",
       "      <td>840</td>\n",
       "      <td>596</td>\n",
       "      <td>437.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>401</td>\n",
       "      <td>940</td>\n",
       "      <td>71.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1668</td>\n",
       "      <td>1014</td>\n",
       "      <td>862</td>\n",
       "      <td>585</td>\n",
       "      <td>447.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>397</td>\n",
       "      <td>940</td>\n",
       "      <td>71.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1730</td>\n",
       "      <td>1051</td>\n",
       "      <td>792</td>\n",
       "      <td>577</td>\n",
       "      <td>471.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>415</td>\n",
       "      <td>975</td>\n",
       "      <td>74.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1830</td>\n",
       "      <td>995</td>\n",
       "      <td>832</td>\n",
       "      <td>599</td>\n",
       "      <td>456.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>421</td>\n",
       "      <td>1048</td>\n",
       "      <td>78.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  height  chest  waist  thigh  shoulder    arm  top  bottom  \\\n",
       "0         NaN    1778    892    720    533     441.0  569.0  443     979   \n",
       "1         NaN    1702   1021    840    596     437.0  559.0  401     940   \n",
       "2         NaN    1668   1014    862    585     447.0  563.0  397     940   \n",
       "3         NaN    1730   1051    792    577     471.0  591.0  415     975   \n",
       "4         NaN    1830    995    832    599     456.0  627.0  421    1048   \n",
       "\n",
       "   weight  \n",
       "0    60.9  \n",
       "1    71.8  \n",
       "2    71.7  \n",
       "3    74.8  \n",
       "4    78.9  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c08a66c-017f-4ba6-aad4-1188c5595a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_cols = ['chest','waist','thigh', 'shoulder', 'arm', 'top', 'bottom']  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols].values  # 독립 변수\n",
    "Y = df[Y_cols].values  # 종속 변수\n",
    "\n",
    "# 상수항(절편) 추가\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 다변량 선형 회귀 모델 적합\n",
    "model = sm.OLS(Y_train, X_train)\n",
    "result = model.fit()\n",
    "\n",
    "# 훈련된 모델을 사용하여 테스트 데이터 예측\n",
    "Y_pred = result.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2eff163-157b-4594-97c8-3b986c25a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1414, 3)\n",
      "Shape of Y_train: (1414, 7)\n",
      "Shape of X_test: (606, 3)\n",
      "Shape of Y_test: (606, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f74cb35-e03f-4fc5-a372-bc99a7ac2d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다항 회귀 모델 예측 결과:\n",
      "[[ 962.96151826  544.46384556  440.8673387   582.23282708  412.88660334\n",
      "   982.78203118]\n",
      " [ 977.5023014   553.51666885  447.03878688  592.48678916  417.85674451\n",
      "  1002.26166618]\n",
      " [ 952.11161924  535.41853004  443.50707633  589.72916482  416.01486745\n",
      "   998.20224384]\n",
      " ...\n",
      " [ 953.48655336  531.684341    456.19479332  617.12767937  430.21873463\n",
      "  1050.15826493]\n",
      " [1007.63626958  576.22158102  448.22867148  589.50614268  417.22994607\n",
      "   994.22756228]\n",
      " [1045.04292196  603.26515516  459.94583676  608.45637022  426.83518565\n",
      "  1028.38163644]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_cols = ['chest', 'thigh', 'shoulder', 'arm', 'top', 'bottom']  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols].values  # 독립 변수\n",
    "Y = df[Y_cols].values  # 종속 변수\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 다항 회귀 모델 적용\n",
    "poly = PolynomialFeatures(degree=2)  # 2차 다항식 사용 (필요에 따라 degree를 조절할 수 있음)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# 다항 회귀 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, Y_train)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "Y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"다항 회귀 모델 예측 결과:\")\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c62fc5be-9752-4b4e-9d25-5d2adebd199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 476.1681538064673\n",
      "R-squared (R2): 0.5908877954929769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예측 결과와 실제 값 사이의 MSE 계산\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# 결정 계수 R-squared 계산\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f443203c-ffe7-4772-83b5-534ef22d616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다변량 분산 분석 결과:\n",
      "                     Multivariate linear model\n",
      "===================================================================\n",
      "                                                                   \n",
      "-------------------------------------------------------------------\n",
      "               x0           Value  Num DF   Den DF  F Value  Pr > F\n",
      "-------------------------------------------------------------------\n",
      "              Wilks' lambda 0.2455 6.0000 1407.0000 720.8408 0.0000\n",
      "             Pillai's trace 0.7545 6.0000 1407.0000 720.8408 0.0000\n",
      "     Hotelling-Lawley trace 3.0739 6.0000 1407.0000 720.8408 0.0000\n",
      "        Roy's greatest root 3.0739 6.0000 1407.0000 720.8408 0.0000\n",
      "-------------------------------------------------------------------\n",
      "                                                                   \n",
      "-------------------------------------------------------------------\n",
      "           x1            Value   Num DF   Den DF   F Value   Pr > F\n",
      "-------------------------------------------------------------------\n",
      "          Wilks' lambda   0.0036 6.0000 1407.0000 65738.8769 0.0000\n",
      "         Pillai's trace   0.9964 6.0000 1407.0000 65738.8769 0.0000\n",
      " Hotelling-Lawley trace 280.3364 6.0000 1407.0000 65738.8769 0.0000\n",
      "    Roy's greatest root 280.3364 6.0000 1407.0000 65738.8769 0.0000\n",
      "===================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_cols = ['chest', 'thigh', 'shoulder', 'arm', 'top', 'bottom']  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols].values  # 독립 변수\n",
    "Y = df[Y_cols].values  # 종속 변수\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 다변량 분산 분석 (MANOVA)\n",
    "manova = MANOVA(Y_train, X_train)\n",
    "result = manova.mv_test()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"다변량 분산 분석 결과:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49923438-2aef-4f56-8cf3-84517fb5a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8843ac6-43e7-4085-b1a8-09bc359802f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_cols = ['chest', 'thigh', 'shoulder', 'arm', 'top', 'bottom']  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols].values  # 독립 변수\n",
    "Y = df[Y_cols].values  # 종속 변수\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 다변량 회귀 트리 모델 학습 및 예측\n",
    "regressor_models = []\n",
    "Y_pred_reg = []\n",
    "\n",
    "for i in range(Y_train.shape[1]):\n",
    "    model_reg = DecisionTreeRegressor(random_state=42)\n",
    "    model_reg.fit(X_train, Y_train[:, i])\n",
    "    Y_pred_reg.append(model_reg.predict(X_test))\n",
    "    regressor_models.append(model_reg)\n",
    "\n",
    "# 다변량 분류 트리 모델 학습 및 예측\n",
    "classifier_models = []\n",
    "Y_pred_cls = []\n",
    "\n",
    "for i in range(Y_train.shape[1]):\n",
    "    model_cls = DecisionTreeClassifier(random_state=42)\n",
    "    # 임의로 특정 값을 기준으로 이진 분류 예시를 드는 것이므로 다양한 형태로 변형해서 사용해 보세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22fd3d6-b420-4dd6-8db8-ed99f221753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다변량 회귀 트리 예측 결과:\n",
      "chest 예측값: [ 973.  1014.   968.  1000.   973.  1024.  1055.  1100.  1005.   981.\n",
      " 1076.  1015.  1010.  1065.   990.  1060.5  980.  1130.  1058.   944.\n",
      " 1010.  1008.   920.  1090.  1017.   967.  1137.  1031.  1032.  1009.\n",
      "  922.   968.   981.   905.  1111.  1039.  1065.  1025.  1055.  1092.\n",
      "  975.   907.   999.  1000.  1055.  1040.  1058.  1014.   951.   969.\n",
      " 1057.  1090.  1031.  1019.  1053.   934.   995.  1068.  1128.   887.\n",
      "  946.   930.  1053.  1031.   960.  1031.  1098.  1001.   883.  1005.\n",
      "  927.   924.   972.  1040.  1114.  1060.  1025.   965.  1026.  1139.\n",
      "  922.   865.  1019.  1003.  1015.  1100.  1125.  1027.   981.   892.\n",
      " 1026.   921.  1010.  1098.  1095.  1035.   994.   995.  1114.  1004.\n",
      "  899.  1147.   938.  1004.   758.   960.   992.  1079.   984.   936.\n",
      "  909.   985.  1132.  1079.  1145.  1003.  1095.  1125.  1068.  1087.\n",
      " 1005.  1043.  1122.   997.  1040.   948.  1025.   968.  1058.  1027.\n",
      " 1084.  1057.  1031.  1000.  1036.   981.  1122.  1087.   892.   993.\n",
      " 1013.  1139.  1041.   984.   977.  1002.   988.   999.  1018.  1067.5\n",
      " 1004.5 1043.   965.   967.   984.  1070.  1090.  1058.  1068.  1000.\n",
      "  944.   966.  1092.   979.  1155.   860.   986.  1099.   988.  1057.\n",
      "  981.  1009.  1005.   970.  1054.   980.  1059.  1090.  1040.  1122.\n",
      " 1028.   980.  1131.  1166.  1004.  1135.   944.  1095.  1077.   975.\n",
      " 1050.  1014.  1058.   970.   994.  1051.  1004.5  974.   959.  1040.\n",
      " 1075.  1024.   965.   975.  1063.   941.   965.   942.  1105.  1068.\n",
      " 1132.   993.   969.   923.  1020.   942.  1017.  1031.  1010.  1072.\n",
      " 1010.  1058.  1048.   993.   999.   983.  1090.  1125.  1090.   941.\n",
      " 1076.  1065.   981.  1005.   993.   960.  1008.   978.   934.   973.\n",
      "  859.  1013.   984.   958.  1002.   963.  1057.  1071.   988.   931.\n",
      " 1004.  1057.   967.   978.  1025.   966.   892.   994.  1032.  1068.\n",
      " 1084.  1010.   950.  1003.   970.   920.   983.   972.   949.   800.\n",
      " 1040.   960.  1075.  1024.  1050.   965.  1132.   935.   964.  1010.\n",
      " 1040.  1068.  1004.  1018.  1075.   994.  1028.   984.  1054.  1043.\n",
      " 1043.  1095.   969.  1055.   995.  1103.  1024.   852.   975.  1133.\n",
      " 1055.   908.  1017.   899.  1020.   952.  1060.   932.  1049.  1015.\n",
      "  938.  1034.   998.  1035.   975.  1127.   929.   965.  1009.   965.\n",
      "  974.  1017.  1038.  1099.  1003.   905.  1095.  1070.  1043.  1040.\n",
      " 1100.   959.   944.  1115.  1202.  1034.   972.  1115.   988.5 1139.\n",
      " 1038.   992.5  938.  1019.   944.   939.  1106.  1055.   985.  1052.\n",
      "  960.  1076.5 1032.  1025.  1031.  1125.  1030.   986.   991.5 1004.\n",
      "  967.   997.  1040.  1048.  1076.  1137.  1071.  1024.   987.   968.\n",
      " 1093.   997.  1060.  1122.   935.  1019.  1048.  1172.  1148.  1060.\n",
      "  944.   967.  1042.  1122.  1030.   982.  1172.  1000.   975.   953.\n",
      " 1125.  1055.  1030.   972.  1031.  1037.   900.   974.  1082.   988.5\n",
      "  935.  1102.  1035.   977.  1012.  1125.  1009.  1043.   999.  1088.\n",
      "  932.  1010.  1084.  1138.  1104.  1025.  1043.   880.   987.  1122.\n",
      " 1076.5  961.   995.  1008.   978.   970.  1002.  1098.  1048.   988.\n",
      "  994.  1046.   974.  1106.  1170.  1017.  1041.   939.  1223.   970.\n",
      " 1117.  1035.  1122.  1032.  1055.  1062.  1003.  1202.  1108.  1139.\n",
      " 1013.  1005.   997.  1073.   896.   992.   888.  1011.  1029.  1038.\n",
      " 1067.   999.  1145.  1004.  1125.  1042.  1044.   975.  1021.   974.\n",
      " 1010.  1001.  1114.   997.   999.   970.   963.  1055.   907.   993.\n",
      " 1024.  1068.  1108.  1068.   942.  1046.   980.  1094.  1087.  1051.\n",
      "  948.  1138.   980.  1071.  1045.  1002.  1010.  1003.  1027.   973.\n",
      "  988.5  962.  1059.  1065.  1105.  1016.  1004.  1034.  1112.  1240.\n",
      " 1075.   920.   990.   961.   993.  1133.  1078.  1024.  1088.   958.\n",
      " 1002.  1017.  1132.  1043.   931.   962.  1065.   917.  1052.   932.\n",
      " 1039.   905.  1019.  1050.  1039.  1004.  1084.  1015.   934.  1163.\n",
      " 1079.   985.  1113.   926.   939.   893.  1076.   940.   965.  1024.\n",
      "  950.  1094.   993.  1017.  1127.  1015.  1024.   997.  1084.  1094.\n",
      "  987.  1028.  1016.  1145.  1094.  1073.  1055.  1109.  1080.   934.\n",
      "  936.   951.  1060.5 1013.   935.  1040.   999.  1041.  1024.  1050.\n",
      "  993.   984.   960.   920.  1041.  1025.  1122.   955.  1087.  1159.\n",
      " 1011.  1138.  1024.  1005.  1100.  1052.  1039.  1051.   993.  1054.\n",
      "  984.  1108.  1108.  1015.   994.  1076. ]\n",
      "thigh 예측값: [553.  557.  521.  576.  552.  572.  589.  660.  632.  530.  600.  538.\n",
      " 622.  606.  595.  603.  555.  641.  602.  564.  577.  590.  532.  625.\n",
      " 630.  610.  686.  585.  600.5 568.  536.  568.  535.  535.  633.  598.\n",
      " 613.  554.  585.  659.  537.  532.  557.  633.  594.5 605.  613.  569.\n",
      " 525.  539.  575.  592.  606.  572.  596.  532.  571.  632.  647.  520.\n",
      " 516.  540.  596.  594.  607.  585.  621.  546.  530.  521.  574.  556.\n",
      " 600.  590.  675.  652.  615.  532.  610.  590.  555.  496.  667.  570.\n",
      " 560.  663.  639.  649.  535.  520.  632.  524.  585.  651.  626.  637.\n",
      " 521.  555.  675.  559.  527.  727.  530.  595.  480.  544.  605.  616.\n",
      " 581.  525.  550.  547.  637.  600.  617.  585.  617.  639.  615.  614.\n",
      " 582.  588.  651.  572.  568.  502.  551.  521.  606.  610.  599.  593.\n",
      " 606.  542.  593.  546.  688.  614.  538.  536.  612.  651.  585.  525.\n",
      " 580.  551.  532.  548.  529.  592.5 579.5 626.  532.  565.  545.  600.\n",
      " 557.  596.  622.  594.  608.  521.  705.  560.  622.  489.  567.  615.\n",
      " 560.  612.  528.  559.  564.  573.  570.  555.  578.  557.  574.  603.\n",
      " 577.  581.  670.  642.  593.  629.  554.  640.  620.  545.  618.  608.\n",
      " 635.  590.  521.  604.  596.  548.  576.  570.  626.  567.  588.  508.\n",
      " 619.  567.  556.  555.  665.  632.  656.  575.  598.  705.  570.  541.\n",
      " 564.  607.  600.  634.  555.  598.  662.  536.  574.  530.  644.  639.\n",
      " 585.  524.  560.  613.  546.  592.  547.  565.  590.  592.  535.  575.\n",
      " 460.  638.  554.  538.  552.  552.  575.  650.  552.  529.  593.  610.\n",
      " 536.  572.  589.  554.  561.  552.  577.  637.  599.  592.  593.  577.\n",
      " 582.  574.  554.  608.  547.  480.  593.  537.  580.  570.  636.  559.\n",
      " 642.  505.  567.  575.  625.  648.  592.  548.  565.  541.  579.  581.\n",
      " 598.  520.  634.  634.5 580.  632.  590.  594.5 565.  450.  543.  648.\n",
      " 585.  595.  561.  515.  605.  521.  605.  529.  582.  521.  502.  577.\n",
      " 616.  550.  508.  670.  512.  541.  580.  537.  575.  669.  648.  641.\n",
      " 600.  535.  617.  617.  568.  583.  677.  576.  554.  665.  679.  577.\n",
      " 583.  670.  600.5 590.  591.  600.  531.  667.  531.  554.  638.  563.\n",
      " 575.  658.  530.  641.  600.5 554.  573.  688.  611.  571.  590.  537.\n",
      " 610.  568.  590.  611.  612.  645.  650.  587.  554.  598.  628.  572.\n",
      " 627.  648.  556.  592.  694.  654.  683.  585.  564.  560.  588.  650.\n",
      " 630.  497.  675.  618.  599.  548.  651.  613.  585.  608.  568.  585.\n",
      " 520.  616.  698.  600.5 546.  660.  596.  598.  599.  723.  620.  568.\n",
      " 572.  634.  562.  561.  630.  674.  610.  589.  615.  511.  590.  651.\n",
      " 677.  557.  482.  590.  553.  563.  575.  638.  637.  546.  597.  590.\n",
      " 578.  640.  659.  631.  618.  521.  728.  582.  694.  648.  635.  592.\n",
      " 576.  573.  540.  688.  593.  623.  592.  649.  589.  600.  511.  522.\n",
      " 475.  531.  580.  627.  607.  558.  652.  564.  617.  591.  568.  538.\n",
      " 593.  580.  580.  558.  715.  566.  544.  573.  534.  598.  536.  530.\n",
      " 593.  629.  623.  570.  522.  622.  565.  630.  615.  603.  502.  698.\n",
      " 544.  634.  606.  649.  577.  610.  552.  545.  600.5 576.  616.  573.\n",
      " 674.  553.  555.  645.  640.  728.  641.  535.  576.  566.  575.  648.\n",
      " 630.  553.  641.  557.  555.  669.  663.  555.  560.  573.  613.  561.\n",
      " 636.  546.  556.  474.  574.  605.  580.  632.  641.  582.  532.  698.\n",
      " 598.  517.  548.  507.  521.  530.  642.  522.  559.  568.  593.  619.\n",
      " 547.  630.  670.  606.  563.  572.  642.  588.  590.  593.  568.  652.\n",
      " 607.  635.  613.  629.  648.  590.  525.  538.  580.  564.  522.  613.\n",
      " 608.  588.  608.  636.  605.  581.  535.  537.  628.  604.  610.  577.\n",
      " 541.  651.  513.  671.  598.  569.  599.  585.  626.  559.  592.  590.\n",
      " 545.  609.  652.  521.  597.  622. ]\n",
      "shoulder 예측값: [424.  458.  445.  412.  470.  409.  454.  480.  445.  435.  425.  495.\n",
      " 442.  422.  446.  470.  467.  449.  453.  397.  425.  469.  405.  435.\n",
      " 410.  485.  457.  420.  441.  429.  440.  484.  459.  425.  461.  478.\n",
      " 402.  469.  466.  448.  411.  447.  440.  455.  441.  468.  463.  458.\n",
      " 485.  420.  414.  466.  465.  459.5 427.  410.  450.  456.  460.  440.\n",
      " 400.9 425.  423.  425.  481.  447.  442.  465.  417.  463.  436.  493.\n",
      " 450.  451.  462.  486.  474.  415.  452.  449.  425.  436.  482.  433.\n",
      " 425.  480.  447.  477.  459.  445.  444.  450.  449.  472.  492.  454.\n",
      " 449.  435.  482.  412.  443.  474.  441.  478.  432.  438.  463.  501.\n",
      " 420.  403.  435.  459.  429.  472.  443.  423.  444.  453.  466.  442.\n",
      " 435.  441.  452.  471.  462.  493.  429.  433.  469.  518.  437.  390.\n",
      " 450.  479.  448.  444.  442.  442.  429.  433.  465.  476.  481.  451.\n",
      " 486.  452.  415.  413.  461.  471.  451.5 450.  415.  485.  424.  490.\n",
      " 477.  436.  471.  455.  426.  450.  455.  482.  485.  412.  437.  480.\n",
      " 421.  526.  467.  482.  472.  499.  455.  467.  455.  477.  435.  469.\n",
      " 448.  442.  460.  448.  435.  495.  460.  470.  441.  460.  439.  441.\n",
      " 410.  473.  449.  460.  451.5 417.  415.  408.  446.  470.  408.  495.\n",
      " 447.  429.  458.  440.  495.  467.  480.  433.  385.  455.  470.  442.\n",
      " 461.  483.  472.  489.  445.  465.  442.  404.  450.  463.  463.  446.\n",
      " 466.  423.  439.  413.  441.  415.  465.  425.  420.  457.  442.  433.\n",
      " 449.  401.  425.  412.  463.  405.  414.  459.  420.  481.  446.  466.\n",
      " 484.  495.  442.  450.  401.  471.  431.  458.  440.  440.  390.  442.\n",
      " 459.  420.  450.  430.  453.  420.  487.  449.  472.  468.  452.  427.\n",
      " 494.  403.  395.  446.  440.  466.  454.  413.  463.  430.  445.  460.\n",
      " 480.  469.  467.  492.  417.  431.  440.  464.  452.  420.  435.  480.\n",
      " 466.  450.  425.  404.  465.  422.  421.  410.  435.  495.  396.  398.\n",
      " 491.  437.  453.  461.  441.  426.  429.  446.  433.  441.  447.  448.\n",
      " 453.  457.  451.  458.  480.  430.  466.  415.  442.  436.  465.  398.\n",
      " 483.  437.  450.  441.  427.  455.  485.  450.  436.  493.  426.  478.\n",
      " 479.  488.  427.  470.5 441.  469.  478.  461.  413.  420.  451.5 445.\n",
      " 485.  426.  438.  429.  437.  451.  459.  466.  421.  484.  451.  475.\n",
      " 461.  438.  425.  411.  500.  431.  526.  460.  397.  455.  440.  485.\n",
      " 475.  493.  450.  450.  430.  400.  420.  455.  413.  420.  452.  425.\n",
      " 430.  406.  433.  461.  403.  503.  484.  452.  476.  475.  481.  480.\n",
      " 457.  459.  405.  466.  501.  433.  462.  430.  446.  351.  455.  443.\n",
      " 470.  394.  435.  420.  463.  448.  442.  444.  454.  435.  455.  433.\n",
      " 449.  447.  498.  449.  481.  435.  526.  439.  455.  478.  463.  466.\n",
      " 463.  448.  442.  475.  454.  472.  441.  451.  408.  457.  351.  404.\n",
      " 405.  485.  413.  434.  441.  462.  443.  451.  465.  430.  434.  405.\n",
      " 421.  442.  472.  480.  456.  481.  456.  448.  430.  478.  413.  447.\n",
      " 430.  472.  461.  423.  412.  449.  440.  500.  469.  469.  493.  458.\n",
      " 435.  489.  447.  430.  438.  433.  472.  442.  448.  443.  490.  457.\n",
      " 476.  424.  478.  427.  460.  506.  463.  410.  453.  439.  450.  480.\n",
      " 441.  439.5 443.  415.  450.  490.  438.  478.  452.  411.  469.  401.\n",
      " 442.  442.  460.  450.  459.5 518.  481.  425.  450.  453.  443.  433.\n",
      " 432.  405.  449.  423.  435.  390.  437.  396.  425.  476.  390.  467.\n",
      " 463.  437.  435.  446.  444.  475.  432.  436.  455.  472.  467.  443.\n",
      " 433.  473.  455.  461.  454.  453.  403.  423.  470.  460.  404.  470.\n",
      " 420.  477.  426.  452.  462.  420.  418.  410.  488.  428.  460.  409.\n",
      " 446.  420.  453.  437.  444.  484.  454.  405.  501.  404.  443.  470.\n",
      " 424.  457.  458.  495.  462.  477. ]\n",
      "arm 예측값: [579.  582.  591.  548.  582.  601.  662.  651.  578.5 613.  576.  600.\n",
      " 620.  551.  599.  592.  576.  611.  590.  549.  602.  597.  572.  547.\n",
      " 579.  591.  618.  594.  594.  543.  563.  579.  573.  638.  572.  603.\n",
      " 547.  617.  587.  613.  564.  564.  606.  608.  579.  627.  590.  575.\n",
      " 602.  547.  580.  636.  577.  600.  582.  572.  580.  601.  606.  546.\n",
      " 587.1 610.  582.  562.  632.  585.  579.  584.  567.  597.  599.  615.\n",
      " 616.  592.  575.  599.  558.  576.  588.  638.  576.  590.  590.  576.\n",
      " 602.  610.  599.  590.  564.  615.  615.  571.  577.  634.  605.  576.\n",
      " 637.  525.  588.  520.  571.  610.  559.  575.  573.  568.  591.  639.\n",
      " 611.  569.  570.  590.  599.  608.  604.  576.  594.  572.  599.  615.\n",
      " 559.  586.  568.  583.  563.  597.  603.  607.  618.  594.  590.  630.\n",
      " 566.  592.  557.  589.  594.  615.  575.  594.  603.  591.  582.  584.\n",
      " 587.  560.  572.  562.  652.  610.  578.5 596.  576.  590.  603.  609.\n",
      " 587.  599.  618.  602.  552.  652.  543.  584.  630.  579.  608.  608.\n",
      " 559.  649.  576.  584.  605.  611.  583.5 576.  578.  609.  603.  615.\n",
      " 610.  557.  619.  590.  559.  601.  589.  613.  608.  572.  576.  603.\n",
      " 558.  647.  631.  565.  558.  534.  532.  555.  610.  590.  574.  606.\n",
      " 563.  568.  516.  576.  637.  598.  568.  562.  525.  525.  596.  603.\n",
      " 569.  631.  603.  608.  565.  598.  620.  566.  630.  600.  609.  589.\n",
      " 587.  561.  592.  547.  589.  558.  577.  576.  593.  591.  571.  562.\n",
      " 571.  555.  520.  588.  569.  599.  576.  614.  547.  606.  559.  608.\n",
      " 599.  600.  554.  622.  586.  603.  609.  604.  590.  563.  630.  559.\n",
      " 584.  590.  622.  531.  589.  546.  632.  572.  638.  544.  568.  584.\n",
      " 615.  540.  525.  576.  577.  599.  658.  577.  598.  537.  643.  595.\n",
      " 600.  571.  571.  610.  578.  563.  576.5 587.  560.  607.  588.  613.\n",
      " 587.  559.  605.  561.  592.  575.  582.  537.  560.  631.  581.  543.\n",
      " 613.  608.  623.  562.  607.  578.  601.  572.  562.  569.  563.  608.\n",
      " 578.  604.  583.  599.  578.  602.  616.  532.  571.  607.  621.  570.\n",
      " 620.  590.  588.5 620.  601.  553.  602.  572.  578.  558.  565.  603.\n",
      " 589.  572.  559.  590.  593.  617.  604.  565.  578.  605.  583.  555.\n",
      " 591.  544.  603.  629.  590.  586.  614.  623.  554.  624.  586.  623.\n",
      " 594.  591.  595.  523.  589.  636.  659.  592.  549.  568.  586.  630.\n",
      " 651.  574.  610.  608.  606.  572.  561.  608.  560.  547.  578.  582.\n",
      " 571.  573.  568.  579.  540.  597.  618.  591.  582.  640.  649.  578.\n",
      " 548.  568.  548.  560.  602.  568.  601.  552.  596.  560.  647.  583.\n",
      " 590.  540.  519.  593.  597.  641.  562.  564.  576.  572.  576.  613.\n",
      " 571.  658.  610.  577.  597.  553.  679.  597.  622.  599.  609.  606.\n",
      " 650.  593.  566.  618.  637.  571.  573.  592.  552.  580.  560.  575.\n",
      " 586.  626.  562.  572.  611.  575.  604.  629.  586.  543.  551.  559.\n",
      " 579.  601.  588.  641.  607.  623.  568.  609.  558.  600.  569.  576.\n",
      " 623.  574.  596.  572.  569.  659.  576.  589.  604.  615.  597.  585.\n",
      " 561.  638.  577.  582.  606.  576.  585.  557.  594.  586.  599.  557.\n",
      " 580.  590.  572.  543.  639.  653.  620.  527.  650.  590.  651.  608.\n",
      " 597.  572.  651.  540.  562.  563.  572.  600.  576.  568.  555.  565.\n",
      " 549.  500.  605.  607.  555.  612.  561.  578.5 631.  619.  559.  568.\n",
      " 642.  549.  583.  591.  553.  571.  590.  559.  600.  583.  602.  580.\n",
      " 613.  579.  553.  582.  596.  600.  569.  586.  611.  559.  629.  604.\n",
      " 577.  552.  608.  594.  579.  629.  575.  591.  592.  605.  560.  592.\n",
      " 547.  571.  552.  595.  563.  611.  519.  527.  586.  633.  585.  583.\n",
      " 537.  648.  626.  594.  578.  590.  579.  547.  599.  570.  599.  583.\n",
      " 559.  599.  539.  631.  576.  588. ]\n",
      "top 예측값: [390.  415.  418.  366.  391.  402.  426.  421.  423.  420.  411.  443.\n",
      " 440.  397.  392.  399.  416.  416.  412.  388.  413.  417.  390.  408.\n",
      " 440.  415.  423.  405.  432.  419.  408.  425.  412.  437.  427.  452.\n",
      " 427.  420.  434.  428.  410.  401.  409.  414.  423.  459.  403.  415.\n",
      " 405.  392.  389.  434.  389.  406.  428.  388.  405.  405.  452.  390.\n",
      " 422.  427.  428.  401.  446.  404.  430.  432.  408.  413.  392.  428.\n",
      " 406.  401.  411.  425.  401.  385.  395.  430.  388.  418.  476.  424.\n",
      " 408.  421.  448.  460.  425.  430.  443.  395.  445.  430.  439.  446.\n",
      " 427.  362.  411.  434.  412.  445.  446.  417.  447.  426.  396.  444.\n",
      " 421.  403.  386.  433.  443.  399.  437.  416.  400.  409.  437.  413.\n",
      " 393.  415.  435.  389.  418.  392.  418.  406.  408.  415.  411.  390.\n",
      " 438.  419.  402.  381.  422.  413.  382.  407.  390.  406.  400.  394.\n",
      " 428.  412.  431.  424.  424.  412.  412.  459.  385.  451.  413.  409.\n",
      " 437.  440.  447.  419.  430.  449.  425.  443.  439.  421.  403.  439.\n",
      " 417.  438.  388.  435.  436.  426.  448.5 416.  412.  437.  450.  416.\n",
      " 426.  392.  428.  448.  393.  412.  401.  467.  432.  426.  422.  404.\n",
      " 434.  417.  427.  455.  401.  435.  384.  411.  406.  410.  384.  434.\n",
      " 372.  358.  429.  394.  440.  406.  443.  414.  402.  427.  387.  389.\n",
      " 416.  428.  400.  415.  422.  422.  428.  372.  422.  416.  436.  448.\n",
      " 434.  390.  437.  427.  381.  407.  404.  431.  404.  412.  416.  414.\n",
      " 397.  429.  371.  432.  445.  422.  389.  440.  392.  399.  393.  432.\n",
      " 441.  417.  394.  422.  416.  392.  436.  416.  443.  400.  390.  404.\n",
      " 386.  391.  422.  406.  414.  383.  446.  413.  430.  390.  401.  417.\n",
      " 440.  368.  381.  425.  409.  437.  455.  416.  398.  384.  431.  417.\n",
      " 459.  410.  421.  409.  422.  458.  390.  403.  409.  415.  399.  420.\n",
      " 434.  439.  393.  414.  412.  422.  423.  421.  441.  455.  420.  367.\n",
      " 457.  403.  403.  438.  413.  417.  431.  431.  414.  416.  407.  419.\n",
      " 413.  412.  470.  403.  452.  437.  442.  384.  424.  430.  475.  367.\n",
      " 441.  428.  417.  441.  409.  417.  405.  476.  415.  428.  429.  437.\n",
      " 433.  409.  418.  450.5 432.  420.  452.  429.  435.  414.  420.5 425.\n",
      " 423.  390.  452.  445.  451.  454.  440.  390.  371.  427.  447.  448.\n",
      " 415.  394.  422.  390.  432.  418.  478.  437.  375.  459.  415.  439.\n",
      " 421.  428.  398.  407.  420.5 423.  435.  408.  435.  374.  418.  418.\n",
      " 390.  406.  416.  439.  399.  434.  406.  427.  418.  444.  466.  452.\n",
      " 411.  433.  381.  413.  438.  416.  425.  406.  427.  404.  442.  435.\n",
      " 441.  427.  372.  404.  391.  481.  414.  400.  446.  448.  425.  379.\n",
      " 426.  437.  415.  433.  397.  419.  457.  423.  428.  437.  417.  400.\n",
      " 431.  376.  404.  425.  411.  418.  397.  452.  406.  425.  408.  392.\n",
      " 416.  418.  376.  369.  416.  401.  440.  379.  454.  410.  404.  433.\n",
      " 426.  431.  434.  481.  439.  424.  443.  426.  387.  459.  412.  372.\n",
      " 438.  395.  412.  369.  424.  438.  433.  373.  437.  398.  392.  430.\n",
      " 449.  415.  424.  460.  449.  424.  408.  414.  432.  429.  418.  402.\n",
      " 408.  387.  432.  413.  442.  455.  441.  401.  430.  428.  427.  440.\n",
      " 409.  428.  440.  427.  436.  397.  416.  452.  418.  388.  413.  395.\n",
      " 401.  422.  446.  468.  404.  437.  443.  423.  427.  400.  439.  455.\n",
      " 400.  381.  453.  424.  393.  408.  428.  393.  404.  443.  410.  403.\n",
      " 421.  439.  425.  439.  411.  417.  428.  436.  409.  436.  437.  440.\n",
      " 414.  391.  387.  412.  446.  417.  385.  418.  399.  390.  404.  443.\n",
      " 378.  401.  430.  410.  418.  421.  401.  398.  456.  411.  436.  428.\n",
      " 384.  424.  419.  422.  443.  405.  425.  417.  417.  407.  344.  448.5\n",
      " 379.  392.  397.  455.  425.  439. ]\n",
      "bottom 예측값: [1015.   992.   972.   886.   968.  1019.  1066.  1029.   977.5 1011.\n",
      "  951.  1012.  1018.   949.   991.  1024.   961.  1041.   966.   946.\n",
      " 1049.   999.   954.  1013.   960.  1034.  1078.  1015.   975.   982.\n",
      "  981.   980.   983.  1095.   887.  1030.   926.  1049.  1034.  1056.\n",
      "  911.   970.  1042.  1004.   982.  1025.   957.  1040.  1011.   919.\n",
      "  979.  1031.   979.   985.   956.   972.  1002.   964.  1073.   968.\n",
      "  973.  1003.   946.   950.  1036.  1027.5  946.   972.   941.   982.\n",
      "  937.   970.  1098.   998.   983.   981.   984.   989.   965.  1072.\n",
      "  929.  1066.   950.   999.  1049.  1029.   961.   984.   955.   998.\n",
      " 1021.   918.   958.  1078.  1028.   951.  1087.   893.   974.   918.\n",
      "  934.  1006.   938.   920.   947.  1000.   986.  1071.   997.   912.\n",
      "  950.   997.  1001.  1024.  1065.   936.   968.   984.  1017.  1006.\n",
      "  974.   932.   974.  1020.  1002.   997.   950.   982.  1004.  1015.\n",
      "  946.   944.   930.  1001.   922.   956.   946.  1006.   969.   969.\n",
      " 1024.  1067.   974.  1013.  1012.   987.   980.   922.  1090.   966.\n",
      "  977.5  998.   989.  1005.   969.   997.  1046.   977.  1019.  1024.\n",
      "  945.  1066.   920.   954.   991.   973.  1005.  1006.   967.  1031.\n",
      "  964.  1034.   992.  1068.  1007.   991.  1004.  1046.  1032.  1001.\n",
      " 1073.   944.  1062.   988.   994.   990.   997.  1068.   941.   993.\n",
      "  940.  1041.   960.  1083.  1047.   959.   977.5  930.   902.   962.\n",
      " 1069.   987.   936.  1024.   941.   966.   902.   943.  1094.   985.\n",
      " 1028.   944.   922.   920.   975.  1040.   952.  1075.  1026.  1074.\n",
      "  975.  1024.  1039.   965.   992.   952.   962.   988.  1034.   967.\n",
      "  989.   926.   942.   970.   983.   966.  1019.   985.   972.   944.\n",
      "  930.   946.   860.   978.   969.   972.   930.  1041.   882.   974.\n",
      "  994.   980.  1005.  1075.   889.   992.   987.   948.  1037.   968.\n",
      "  956.   941.   995.   944.  1036.   973.  1032.   918.  1016.   935.\n",
      " 1036.  1024.  1072.   960.  1010.   978.  1046.   909.   885.  1003.\n",
      "  986.  1017.  1089.   983.   961.   874.  1035.   997.   992.   980.\n",
      "  970.  1054.   954.   964.   964.   995.   987.   987.  1013.  1074.\n",
      " 1025.   903.  1030.   931.  1018.   965.   964.   911.   994.  1050.\n",
      "  970.   951.  1025.  1005.  1046.   978.  1030.   941.   982.  1017.\n",
      "  944.   952.   949.  1023.   968.   975.   968.   949.   992.  1023.\n",
      " 1029.   902.   972.   960.  1039.   916.  1062.  1075.  1027.5 1058.\n",
      "  972.   999.  1011.   954.   968.   997.   946.  1027.   992.   984.\n",
      "  949.  1075.   975.  1050.  1030.   978.   968.  1015.   971.5  962.\n",
      " 1011.   922.   975.   972.  1075.  1002.  1041.   960.   922.   980.\n",
      "  980.  1047.  1007.  1001.   970.   870.  1015.  1069.  1082.   995.\n",
      "  946.   995.   957.   991.  1029.   997.  1042.  1031.   961.   978.\n",
      "  987.  1043.   968.   938.  1012.   961.   953.   948.   979.  1014.\n",
      "  907.  1003.  1037.   989.   939.  1043.  1088.  1032.   908.  1009.\n",
      "  904.   948.  1003.   979.   973.   945.   998.   888.  1083.   973.\n",
      " 1075.   907.   893.  1019.  1029.  1066.   944.   963.   965.   945.\n",
      "  980.  1074.  1009.  1094.  1021.  1008.   994.   961.  1139.  1036.\n",
      "  986.  1030.   996.   962.  1014.   996.   963.  1017.  1071.   960.\n",
      "  998.   998.   945.   944.   887.   887.   966.  1049.   926.   977.\n",
      " 1041.   968.  1081.  1074.   971.   890.   959.   943.   940.   982.\n",
      " 1026.  1066.  1007.  1047.   949.  1018.   983.  1030.   983.   938.\n",
      " 1023.  1042.  1069.   896.   965.  1079.   943.   965.  1032.  1013.\n",
      "  997.   979.   949.  1041.   960.   976.  1003.   999.  1010.   906.\n",
      "  987.   936.   983.   918.   960.   994.   947.   951.  1035.  1078.\n",
      " 1048.   906.   997.  1001.   989.  1006.   908.   945.  1080.   907.\n",
      "  929.   932.   974.  1032.   999.   995.   930.   965.   902.   907.\n",
      " 1037.   981.   985.  1015.   986.   977.5 1029.  1043.   936.   964.\n",
      "  986.   904.   988.  1040.   961.   926.   948.   980.  1013.   999.\n",
      "  937.   976.  1007.   943.   987.   951.   982.  1068.   949.   948.\n",
      " 1019.   937.  1036.  1065.   977.   908.  1043.  1030.   965.  1047.\n",
      "  969.  1009.  1024.  1024.   896.   960.   902.   991.   945.  1008.\n",
      "  995.   997.   939.   906.  1003.  1046.   959.   994.   874.  1062.\n",
      " 1041.   940.   969.   988.   965.   948.  1030.   908.   941.   977.\n",
      "  969.  1021.   949.  1047.   980.  1035. ]\n",
      "\n",
      "다변량 분류 트리 예측 결과:\n",
      "chest 예측값: [0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0\n",
      " 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0\n",
      " 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
      " 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1\n",
      " 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1\n",
      " 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0\n",
      " 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
      " 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1\n",
      " 1 0 0 1 1 0 0 1 0 1 1 0 0 1]\n",
      "thigh 예측값: [0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1\n",
      " 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
      " 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1\n",
      " 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1\n",
      " 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
      " 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
      " 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0\n",
      " 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0\n",
      " 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1\n",
      " 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1\n",
      " 0 0 1 1 1 0 1 1 1 1 1 0 1 1]\n",
      "shoulder 예측값: [0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1\n",
      " 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
      " 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1\n",
      " 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1\n",
      " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1\n",
      " 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
      " 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1\n",
      " 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0\n",
      " 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1\n",
      " 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 0 1 1 0 0 1 1 1 1]\n",
      "arm 예측값: [1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0\n",
      " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1\n",
      " 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0\n",
      " 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0\n",
      " 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1\n",
      " 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1\n",
      " 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0\n",
      " 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0\n",
      " 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
      " 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
      " 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
      " 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1\n",
      " 0 0 0 0 1 0 0 0 0 1 0 1 0 1]\n",
      "top 예측값: [0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
      " 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0\n",
      " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
      " 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
      " 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0\n",
      " 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0\n",
      " 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0\n",
      " 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1\n",
      " 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1\n",
      " 1 0 1 1 1 1 0 1 0 0 0 1 1 1]\n",
      "bottom 예측값: [1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0\n",
      " 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0\n",
      " 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
      " 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1\n",
      " 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1\n",
      " 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0\n",
      " 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1\n",
      " 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
      " 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
      " 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0\n",
      " 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0\n",
      " 0 1 0 0 1 0 1 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_cols = ['chest', 'thigh', 'shoulder', 'arm', 'top', 'bottom']  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols].values  # 독립 변수\n",
    "Y = df[Y_cols].values  # 종속 변수\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 다변량 회귀 트리 모델 학습 및 예측\n",
    "regressor_models = []\n",
    "Y_pred_reg = []\n",
    "\n",
    "for i in range(Y_train.shape[1]):\n",
    "    model_reg = DecisionTreeRegressor(random_state=42)\n",
    "    model_reg.fit(X_train, Y_train[:, i])\n",
    "    Y_pred_reg.append(model_reg.predict(X_test))\n",
    "    regressor_models.append(model_reg)\n",
    "\n",
    "# 다변량 분류 트리 모델 학습 및 예측\n",
    "classifier_models = []\n",
    "Y_pred_cls = []\n",
    "\n",
    "for i in range(Y_train.shape[1]):\n",
    "    model_cls = DecisionTreeClassifier(random_state=42)\n",
    "    model_cls.fit(X_train, (Y_train[:, i] > Y_train[:, i].mean()).astype(int))  # 예시로 평균을 기준으로 이진 분류\n",
    "    Y_pred_cls.append(model_cls.predict(X_test))\n",
    "    classifier_models.append(model_cls)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"다변량 회귀 트리 예측 결과:\")\n",
    "for i in range(len(Y_cols)):\n",
    "    print(f\"{Y_cols[i]} 예측값: {Y_pred_reg[i]}\")\n",
    "\n",
    "print(\"\\n다변량 분류 트리 예측 결과:\")\n",
    "for i in range(len(Y_cols)):\n",
    "    print(f\"{Y_cols[i]} 예측값: {Y_pred_cls[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5064de8e-9f39-46af-be17-abe1b398c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다변량 선형 회귀 예측 결과:\n",
      "[[ 965.49965203  545.77910295  440.91720607  582.25323426  413.52136504\n",
      "   982.74886885]\n",
      " [ 977.70851733  554.1440432   446.82991983  592.5886001   418.16275214\n",
      "  1002.49574808]\n",
      " [ 956.4249847   538.02355881  443.65640028  589.51682431  416.21293592\n",
      "   998.7517022 ]\n",
      " ...\n",
      " [ 956.98172121  535.72059866  456.37761153  616.07407441  427.14374171\n",
      "  1053.20112184]\n",
      " [1004.24802204  574.90943414  447.70742012  589.96696792  417.94284738\n",
      "   993.91982667]\n",
      " [1040.77356881  601.18406087  459.60297411  608.74988497  426.84103406\n",
      "  1028.08101754]]\n",
      "\n",
      "Mean Squared Error (MSE): 489.9121753868166\n",
      "\n",
      "회귀 계수 (모델의 기울기):\n",
      "[[ 5.36638318 -0.11977307]\n",
      " [ 4.11893143 -0.12785575]\n",
      " [ 0.54807253  0.15440369]\n",
      " [ 0.24638118  0.34360216]\n",
      " [ 0.27460505  0.13732259]\n",
      " [-0.14012311  0.71975844]]\n",
      "\n",
      "절편 (모델의 y 절편):\n",
      "[ 824.54856542  499.77875008  138.07450555  -28.51223923  157.99409749\n",
      " -254.05899629]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_cols = ['chest', 'thigh', 'shoulder', 'arm', 'top', 'bottom']  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols]  # 독립 변수\n",
    "Y = df[Y_cols]  # 종속 변수\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 다변량 선형 회귀 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(\"다변량 선형 회귀 예측 결과:\")\n",
    "print(Y_pred)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(f\"\\nMean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# 회귀 계수 (모델의 기울기) 출력\n",
    "print(\"\\n회귀 계수 (모델의 기울기):\")\n",
    "print(model.coef_)\n",
    "\n",
    "# 절편 (모델의 y 절편) 출력\n",
    "print(\"\\n절편 (모델의 y 절편):\")\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "639bb5ce-a3e1-40ac-8f0a-8f61b4a45ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선형 회귀 예측 결과:\n",
      "[1005.53234178  944.81384599 1039.42674284 1075.28483943  957.04589589\n",
      " 1159.99025114  938.18321944 1066.36715224 1039.23939503  992.68121449\n",
      " 1002.96719754 1010.10456192 1022.55938982  972.99241019 1038.41326718\n",
      "  950.1174831   921.38482563  976.78775906  953.54228423  998.44423399\n",
      " 1061.26849163 1105.53961153 1115.65604826  964.45322091 1013.73433956\n",
      " 1130.15943125  966.23561766 1112.36743721  960.25172131  984.95035157\n",
      "  988.80100605  973.60561136 1013.85065421 1109.68597831 1052.4809455\n",
      "  979.11629891  955.12938299 1074.87868871 1035.90930477 1114.96801164\n",
      " 1109.06690091 1071.13259642 1110.37608887  938.44937778  996.58700179\n",
      " 1088.1162641   981.36205314  963.74548166 1055.02828826 1020.45172682\n",
      " 1041.18736302  964.65042004  999.12639437 1008.94124257  947.81659605\n",
      " 1049.77183379  929.70313806 1168.65958148  986.19455539  965.40361358\n",
      " 1067.82618388 1009.54061734  962.9095025  1033.51180571  964.33500511\n",
      "  958.86959905  993.78127806  938.89890886 1028.86665127  944.26372781\n",
      "  986.04073662  939.54961415 1014.17989555 1090.47435789  965.21626577\n",
      "  983.31002113  979.69597104  898.22991379  976.65571686  889.09929992\n",
      " 1021.3704916  1040.75165834 1023.43477425 1048.45486846  960.39171369\n",
      " 1085.99477469  944.81972222 1061.16600338 1263.40849561 1089.41560074\n",
      " 1016.08258204 1049.12717753 1042.44729439  979.38055611 1060.60015765\n",
      " 1097.10498445 1045.3613826  1154.61160579 1078.67801267 1125.26384601\n",
      "  959.71145445  957.58408882 1048.05459397  958.78283835 1013.83682781\n",
      " 1058.52792483  970.1158261   907.81610777 1062.46326608 1050.86411999\n",
      " 1031.53428377 1005.72746698  951.85840064 1017.68748229 1048.96160629\n",
      " 1030.85402453 1035.35918659 1045.20756383 1061.16997847 1000.40602838\n",
      " 1031.69985501 1015.47923219  984.31554662 1008.24335464 1027.15718882\n",
      "  957.28250029 1009.06948248 1082.1619217  1027.73098472  976.2160371\n",
      "  984.96227682  955.22599502  966.10945169  955.95153576 1032.24997319\n",
      "  999.66856238 1009.12859038 1037.60096573  959.9776128  1055.87014365\n",
      " 1024.70058185 1073.8633119  1005.18132388 1058.33470079 1042.19496244\n",
      " 1004.78899956 1101.17444184 1032.05087291 1031.23649753 1005.15574501\n",
      "  974.08279524 1039.11720414 1058.41161018  975.69754681  962.48364914\n",
      " 1010.42395194  981.01501032 1029.36146384  999.12846831 1003.27086001\n",
      "  945.54526296 1032.59891716 1058.79805827  985.93427328  981.0189854\n",
      " 1031.99954239  973.69237206 1032.05277406  916.45381021 1047.7509315\n",
      "  948.43377231 1064.57887925  960.83139345 1043.24974452 1135.88304548\n",
      "  943.45142636  959.70557822  903.37195475 1006.93814176  979.98183203\n",
      " 1069.85296241 1012.96144338 1070.97877764 1091.05195609 1008.40892588\n",
      " 1262.93926191 1295.22478762  989.90711865 1002.12931723 1004.87973535\n",
      " 1008.95316783  977.02833855  909.6201083  1004.22315383  970.46079498\n",
      " 1018.27303066  913.08431469 1078.9481461  1045.88574913 1100.36991777\n",
      " 1026.22269649  967.95095634  922.93649422  982.32817336  983.16207859\n",
      " 1098.83017444 1107.84447366  983.37898035 1079.55149595 1026.1596135\n",
      "  978.18370773  914.28306422 1038.98308799 1027.0014689  1015.12233805\n",
      " 1034.78936578  975.98340779  967.16233262 1087.78892391 1005.86158312\n",
      "  961.06799784 1066.43023522  953.21494404  982.27286775 1003.1840993\n",
      " 1016.74901488 1059.56300427  963.26242155  925.357671   1034.40499163\n",
      "  999.39065157 1027.46672752 1024.21942289 1089.3091374  1011.69373463\n",
      " 1046.81436523 1072.88734036 1023.53536135 1040.72400554  958.85974774\n",
      " 1042.84549494 1038.78398772 1034.11118047  983.40663315  959.02722012\n",
      "  948.73933592  987.70474477 1043.69927559 1036.74718508  945.13513715\n",
      " 1073.95992392 1064.75827689  981.99495694 1066.99417981 1001.30318938\n",
      " 1045.13462952  945.59659348  965.44301885  984.48509295  956.84472167\n",
      "  978.91719864  977.96490482 1031.92660809 1236.02650531  969.05706894\n",
      " 1024.32986131 1385.42118425  952.8857027   955.31085458 1004.63327963\n",
      "  870.21501969 1084.07238557  955.38966511 1006.98342326 1029.93923483\n",
      "  967.1977628  1070.16250111  953.47125107 1129.46534561  938.135864\n",
      "  996.65405986 1014.87985742 1028.9316354   992.058162    994.01995639\n",
      "  963.8657714  1172.72506379 1083.6643337   975.84929165 1011.29346014\n",
      " 1106.11738252 1011.75871876 1008.33201649  962.73010485 1037.28745195\n",
      "  988.79115474  946.25110107  973.62531399 1002.69706411 1101.08370605\n",
      " 1160.99975172  987.20785826 1043.30885242  926.51304018 1031.27400165\n",
      " 1074.87281248  934.8629805  1019.74588871  977.31229838  943.56376593\n",
      " 1005.64468136 1051.21513789 1087.81657671 1123.31000179 1041.16766038\n",
      "  970.70932463 1127.73825447  994.88739065  887.85717003 1035.38096316\n",
      " 1004.99795115 1015.95831721 1040.19566393 1054.70492315 1002.02873012\n",
      "  988.46381455  983.90542081 1031.36076235  987.76195153 1101.92158636\n",
      "  974.82008844 1118.97628719 1088.69196114 1201.68637548  935.92571275\n",
      "  933.57747026 1007.0760602  1033.39946614  982.77960558  954.96381175\n",
      "  980.30122204 1000.02753046 1041.83011814 1044.92965301  981.07411822\n",
      "  977.60213445 1034.77173708 1020.24467637 1013.08760935 1052.51049945\n",
      " 1006.84740597  982.51742232 1132.42091303  907.81800891 1063.78230536\n",
      "  979.49289568 1063.65613938 1118.50117725  977.19200864 1130.7489547\n",
      " 1213.99343362  992.22770833 1110.95368706 1083.82990494  993.08736521\n",
      " 1017.85305353 1025.599644   1001.74874537 1029.66115122 1096.34193959\n",
      " 1017.41337378 1033.31063149  983.82851142 1081.15051998  998.82480584\n",
      "  977.7184491  1110.21639386  929.03877916  944.12373543 1078.58537573\n",
      "  979.97993088  980.69752145  958.30375332 1078.54199538  921.9980268\n",
      " 1003.00470166 1095.83520176  934.87870805  920.26886172 1003.71831714\n",
      "  974.43761544  987.45051168 1088.11816524 1084.38002312 1074.33254561\n",
      " 1009.24092995 1088.60520044 1013.39714805  985.25401404  963.68239867\n",
      "  981.34822674  899.07574427 1054.22583813  929.30096242 1040.8956258\n",
      " 1032.85919927 1032.94595998 1036.43574524  956.67517535 1066.24098626\n",
      "  971.24164133  982.39903373  982.61403434 1069.3837287   957.79511435\n",
      " 1003.94714416 1095.03672671 1031.54016    1023.49785723 1032.33673389\n",
      "  988.83453509  923.95982119  996.85713522  975.68372041  981.77995633\n",
      "  999.23493165 1061.75155175  988.42838437 1035.90930477 1031.24047261\n",
      " 1052.82591438  974.08867147 1025.01599678 1103.17961659 1074.59870396\n",
      "  966.17650976  980.91442321 1028.76208907 1034.8406963  1013.8999108\n",
      " 1043.00916503 1032.77641365  945.27512953  968.98015955  966.11930301\n",
      " 1020.97021711 1083.1911249   992.32829543  955.6124431  1016.79429638\n",
      "  985.30724571  978.99998426  969.81216363  943.40009584 1042.63256826\n",
      " 1087.6568817   992.66738808  973.76133128 1064.94355077 1009.61562558\n",
      " 1048.72690304  993.45411066 1136.62051148 1050.87604524 1021.47695494\n",
      "  941.55669004 1007.58279803 1008.50951298 1003.03028053  994.46551238\n",
      " 1006.06465848 1101.1409128  1013.89800965  997.46428736 1060.21958579\n",
      "  969.13778062 1010.8774581  1071.39685362 1076.27066229 1113.73763422\n",
      " 1034.06780012 1009.59592295 1017.32073684  997.86266071  933.64055325\n",
      " 1037.95785987 1006.87695992  955.85492373  960.29112658 1001.94991959\n",
      "  957.9705369  1063.85921474 1086.6276785  1076.32786904 1118.47749954\n",
      "  983.36912903  996.69744021  991.12557082 1036.12223145  968.9367792\n",
      " 1036.46339804 1030.52478319  949.96366432  922.35094585 1015.92478818\n",
      " 1074.63620808  980.72517425 1014.58604627  909.4426118   996.68361381\n",
      " 1045.86414535 1004.97237229 1009.99999973 1121.85097014 1229.28734148\n",
      " 1025.12643521 1011.20272435  985.96970346  974.49879728 1040.86399791\n",
      " 1005.18132388  914.55112372 1014.08915976 1053.49234722 1002.31649225\n",
      "  944.66987853  954.32693287 1010.34704255 1017.27735649  958.80651607\n",
      " 1020.96434088  969.53805511 1056.67259378 1031.53030868  948.00584501\n",
      " 1028.15096184 1035.36316168 1050.00256196 1200.2767732  1001.06848613\n",
      " 1063.18293059  989.24068581 1068.28159119 1042.43744307 1079.11181619\n",
      " 1100.10185828 1054.75417974 1083.51639115 1004.8816365  1041.93865541\n",
      "  965.80578922  957.03794572  927.34314312 1070.17045128 1069.98310347\n",
      "  983.17988007  956.03829646  993.76555051 1030.04172308 1006.89078633\n",
      "  948.08085326  925.34194346 1009.65105576 1004.62152717 1001.39980141\n",
      "  988.36720253 1019.72808722 1014.22915213  960.90035266 1027.85507675\n",
      " 1033.647823   1026.62487212  994.41228071 1007.63015347 1060.33590045\n",
      " 1047.02539076  983.53677421  909.29086696 1049.11922736 1081.58415071\n",
      " 1083.88711169 1077.08693882  985.99735626  942.73763809  963.55813385\n",
      " 1015.86568028  989.9643254  1054.87844456 1139.51092197  889.50545064\n",
      " 1026.49473107 1038.70897948 1077.51676726 1021.44930213 1082.92496656\n",
      " 1044.48392423 1185.8226468   972.84654158 1107.37523995  961.34003242\n",
      " 1011.43932875  922.40020244 1167.18482228 1079.91426632 1074.66973712\n",
      "  984.00600792]\n",
      "\n",
      "Mean Squared Error (MSE): 1112.2799723350975\n",
      "\n",
      "회귀 계수 (모델의 기울기):\n",
      "[ 5.46143097 -0.14984369]\n",
      "\n",
      "절편 (모델의 y 절편):\n",
      "869.7466503731555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_col = 'chest'  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols]  # 독립 변수\n",
    "Y = df[Y_col]  # 종속 변수\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 선형 회귀 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(\"선형 회귀 예측 결과:\")\n",
    "print(Y_pred)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(f\"\\nMean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# 회귀 계수 (모델의 기울기) 출력\n",
    "print(\"\\n회귀 계수 (모델의 기울기):\")\n",
    "print(model.coef_)\n",
    "\n",
    "# 절편 (모델의 y 절편) 출력\n",
    "print(\"\\n절편 (모델의 y 절편):\")\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f85c15-241d-4c8d-8958-c4ea87af8e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>height</th>\n",
       "      <th>chest</th>\n",
       "      <th>waist</th>\n",
       "      <th>thigh</th>\n",
       "      <th>shoulder</th>\n",
       "      <th>arm</th>\n",
       "      <th>top</th>\n",
       "      <th>bottom</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1778</td>\n",
       "      <td>892</td>\n",
       "      <td>720</td>\n",
       "      <td>533</td>\n",
       "      <td>441.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>443</td>\n",
       "      <td>979</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1702</td>\n",
       "      <td>1021</td>\n",
       "      <td>840</td>\n",
       "      <td>596</td>\n",
       "      <td>437.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>401</td>\n",
       "      <td>940</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1668</td>\n",
       "      <td>1014</td>\n",
       "      <td>862</td>\n",
       "      <td>585</td>\n",
       "      <td>447.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>397</td>\n",
       "      <td>940</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1730</td>\n",
       "      <td>1051</td>\n",
       "      <td>792</td>\n",
       "      <td>577</td>\n",
       "      <td>471.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>415</td>\n",
       "      <td>975</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1830</td>\n",
       "      <td>995</td>\n",
       "      <td>832</td>\n",
       "      <td>599</td>\n",
       "      <td>456.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>421</td>\n",
       "      <td>1048</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1675</td>\n",
       "      <td>952</td>\n",
       "      <td>882</td>\n",
       "      <td>582</td>\n",
       "      <td>387.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>427</td>\n",
       "      <td>935</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1738</td>\n",
       "      <td>1009</td>\n",
       "      <td>853</td>\n",
       "      <td>584</td>\n",
       "      <td>475.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>423</td>\n",
       "      <td>988</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1614</td>\n",
       "      <td>999</td>\n",
       "      <td>928</td>\n",
       "      <td>608</td>\n",
       "      <td>458.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>429</td>\n",
       "      <td>884</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1818</td>\n",
       "      <td>1053</td>\n",
       "      <td>937</td>\n",
       "      <td>573</td>\n",
       "      <td>433.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>436</td>\n",
       "      <td>1046</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1653</td>\n",
       "      <td>927</td>\n",
       "      <td>802</td>\n",
       "      <td>548</td>\n",
       "      <td>400.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>398</td>\n",
       "      <td>927</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2020 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  height  chest  waist  thigh  shoulder    arm  top  bottom  \\\n",
       "0            NaN    1778    892    720    533     441.0  569.0  443     979   \n",
       "1            NaN    1702   1021    840    596     437.0  559.0  401     940   \n",
       "2            NaN    1668   1014    862    585     447.0  563.0  397     940   \n",
       "3            NaN    1730   1051    792    577     471.0  591.0  415     975   \n",
       "4            NaN    1830    995    832    599     456.0  627.0  421    1048   \n",
       "...          ...     ...    ...    ...    ...       ...    ...  ...     ...   \n",
       "2015         NaN    1675    952    882    582     387.0  578.0  427     935   \n",
       "2016         NaN    1738   1009    853    584     475.0  651.0  423     988   \n",
       "2017         NaN    1614    999    928    608     458.0  583.0  429     884   \n",
       "2018         NaN    1818   1053    937    573     433.0  648.0  436    1046   \n",
       "2019         NaN    1653    927    802    548     400.0  583.0  398     927   \n",
       "\n",
       "      weight  \n",
       "0       61.0  \n",
       "1       72.0  \n",
       "2       72.0  \n",
       "3       75.0  \n",
       "4       79.0  \n",
       "...      ...  \n",
       "2015    66.0  \n",
       "2016    71.0  \n",
       "2017    66.0  \n",
       "2018    81.0  \n",
       "2019    57.0  \n",
       "\n",
       "[2020 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 전체값을 1의 자리에서 반올림\n",
    "df = df.round()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5f11d1c-6f34-4373-890d-a16ecd2103f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선형 회귀 예측 결과:\n",
      "[ 94  99  94 101 100  99 103 109 100  99 105  99 104 105 100 104 100 108\n",
      " 104 100  99  99  95 104 105 104 113 104 104  99 100  99  95  93 110  99\n",
      " 100  99 104 108  95 100  99 104 104  99 104  99  94  95 100 104 100 100\n",
      " 105  95  99 104 114  95  94  94 105 100 104 104 105  99  95  94 100  94\n",
      "  98 104 109 109 104  95 104 108 100  94 109 104  99 109 109 104  95  94\n",
      " 104  95 104 108 109 104  99  96 109  95  95 114  95 100  90  99  99 103\n",
      "  99  95  95  99 109 104 108 100 104 109 104 104 100 104 109  99 105  94\n",
      " 100  94 104 104 105 100 100  99 100 100 110 104  94  95 104 109 105  99\n",
      "  99  99  94 100  98 104 100 104  95  99  99 104 104 104 104 104 100  98\n",
      " 105  99 114  94  99 109 100 108 100  99  99 103 104 100 104 104 104 104\n",
      " 103 100 113 114 100 109  94 108 104  99 105 104 105  99  99 104 100 100\n",
      " 101  99 109  99 100  99 105  95  95 100 113 104 109 100 100 110  99  99\n",
      " 104 109 104 104 100 104 109  95  99  95 104 109 104  95 104 100 100 100\n",
      "  99  99 104  99  95 100  90 110  95  95  99  99 100 104 100  94 100 104\n",
      "  99 103 100  99  94 100 104 104 105 105 100 100  99  95  99 100  99  90\n",
      " 104  99 103 100 104  99 119  95  96  99 104 104 103  99 105 101 104  99\n",
      "  99 104 104 109  99 104  99 104  99  90  99 109 104  95  99  95  99  95\n",
      " 100  95 100  99  95 105 103  99  94 110  94 100  99  99 100 104 105 109\n",
      " 100  94 104 105  99 104 109 101  95 114 113 105  99 113  99 108 105 104\n",
      "  94 109  95  94 105  99  99 109  95 109 104  99  99 110 104  99 104  99\n",
      " 104 100 104 104 109 109 104 100  95  99 109 103 109 109  94 100 104 113\n",
      " 113 104 100  99 104 109 109  94 113 104 104  94 109 104 104 100  99 100\n",
      "  95 100 109  99  95 109 104  99 105 119 108  99 100 109  96 100 109 109\n",
      " 109 100 104  90  99 109 109  95  95 104  99 103 100 110 104  95  99  99\n",
      "  99 108 114 104 105  95 123  99 114 104 104 105  99 104 100 114 103 109\n",
      " 105 104 100 105  90  95  89  94 100 105 108 100 108  99 109 100 105  95\n",
      " 100  99 104 103 114 103  99  99 105  99  95  95 104 109 109 100  95 108\n",
      " 100 104 109 104  94 109  95 104  99 104  99 104 104 100  99  99 109 100\n",
      " 109  99 100 105 108 128 109  95  99  99  99 109 104 100 108  95 100 105\n",
      " 109  99  94  99 100  95 105  95  99  94 100  99 104 100 109  99  95 114\n",
      " 104  96 104  94  95  95 105  95 100  99 100 105  99 105 109  99  99 103\n",
      " 105 104  99 100 104 108 104 104 104 109 104  99  95  94 104 104  95  99\n",
      " 100  99 100 104 105  99  95  95 109 104 104  99 101 108  94 110  99  99\n",
      " 104 105 109 100 100 104  99 109 105  99  99 104]\n",
      "\n",
      "Mean Squared Error (MSE): 12.125789579376862\n",
      "\n",
      "회귀 계수 (모델의 기울기):\n",
      "[ 4.96202748 -0.08915677]\n",
      "\n",
      "절편 (모델의 y 절편):\n",
      "80.0817291084121\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 전체 데이터를 10으로 나눈 뒤 일의 자리에서 반올림\n",
    "df = (df / 10).round()\n",
    "\n",
    "df _col = 'chest'  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols]  # 독립 변수\n",
    "Y = df[Y_col]  # 종속 변수\n",
    "\n",
    "# 데이터 분할 (훈련 데이터 70%, 테스트 데이터 30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 선형 회귀 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(\"선형 회귀 예측 결과:\")\n",
    "print(Y_pred.round().astype(int))  # 예측값을 정수형으로 반올림하여 출력\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "print(f\"\\nMean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# 회귀 계수 (모델의 기울기) 출력\n",
    "print(\"\\n회귀 계수 (모델의 기울기):\")\n",
    "print(model.coef_)\n",
    "\n",
    "# 절편 (모델의 y 절편) 출력\n",
    "print(\"\\n절편 (모델의 y 절편):\")\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a43459e0-4f0a-42bc-b518-d8e3afae4cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "체중을 입력하세요 (kg):  80\n",
      "키를 입력하세요 (cm):  170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측된 가슴 둘레: 1256.83 cm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 엑셀 파일에서 데이터 읽기\n",
    "file_path = './최종데이터.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df = df/100\n",
    "\n",
    "# 독립 변수 (X)와 종속 변수 (Y) 선택\n",
    "X_cols = ['weight', 'height']  # 독립 변수 컬럼 이름\n",
    "Y_col = 'chest'  # 종속 변수 컬럼 이름\n",
    "\n",
    "X = df[X_cols]  # 독립 변수\n",
    "Y = df[Y_col]  # 종속 변수\n",
    "\n",
    "# 선형 회귀 모델 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# 사용자로부터 입력값 받기\n",
    "weight = float(input(\"체중을 입력하세요 (kg): \")) / 100\n",
    "height = float(input(\"키를 입력하세요 (cm): \")) / 100\n",
    "\n",
    "# 입력값을 모델에 적용하여 예측\n",
    "new_data = pd.DataFrame({'weight': [weight],\n",
    "                         'height': [height]})\n",
    "predicted_chest = model.predict(new_data)\n",
    "\n",
    "print(f\"예측된 가슴 둘레: {predicted_chest[0]:.2f} cm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c8e53-3e2a-45ad-b2ee-d60ea8a72d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
